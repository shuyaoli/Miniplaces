{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 10\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.43404748, 0.42429799, 0.40133824), \n",
    "                            (0.26775341, 0.26286543, 0.28041377))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniplacesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, desc_file, root_dir, transform = T.ToTensor()):\n",
    "        \n",
    "        all_data = pd.read_csv(desc_file, delimiter=' ', header = None)\n",
    "        self.desc_frame = all_data.loc[all_data.iloc[:, 1].isin(range(0, NUM_CLASS))]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.desc_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_names = os.path.join(self.root_dir, self.desc_frame.iloc[idx, 0])\n",
    "        images = io.imread(img_names) / 255\n",
    "        \n",
    "        images = self.transform(images)\n",
    "            \n",
    "        return (images, self.desc_frame.iloc[idx,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MiniplacesDataset(desc_file='data/train.txt', root_dir='images/', transform = transform)\n",
    "val_data = MiniplacesDataset(desc_file='data/val.txt', root_dir='images/', transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(train_data, batch_size= 32, shuffle = True)\n",
    "loader_val = DataLoader(val_data, batch_size= 32, shuffle = True)\n",
    "# loader_test = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, verbose = False):\n",
    "    print('Checking accuracy on validation set')\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"Printing first 10 results in each batch\")\n",
    "            print(\"Groud truth: \", y[:20])\n",
    "            print(\"Predicted: \", preds[:20])\n",
    "            \n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1, verbose=False, lr_decay = 0.9):\n",
    "    \"\"\"    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, lr_decay)\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "    \n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_val == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy(loader_val, model, verbose)\n",
    "                print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] \n",
    "    return x.view(N, -1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel_1 = 16\n",
    "# channel_2 = 8\n",
    "\n",
    "# hidden_dim = 100\n",
    "\n",
    "# learning_rate =  1e-3 # 2e-4 is too small\n",
    "# reg = 1e-6\n",
    "\n",
    "# print_loss = 50\n",
    "\n",
    "# print_val = 100\n",
    "\n",
    "# (size_0, size_1, size_2) = (3, 9, 81)\n",
    "\n",
    "# loss_history = [] \n",
    "\n",
    "# model = nn.Sequential(\n",
    "    \n",
    "#     nn.AvgPool2d(2),\n",
    "    \n",
    "#     nn.Conv2d(3, 32, 3, padding=1, bias=True),\n",
    "#     nn.ReLU(),\n",
    "#     nn.BatchNorm2d(32),\n",
    "#     nn.Dropout(),\n",
    "#     nn.MaxPool2d(2),\n",
    "    \n",
    "#     nn.Conv2d(32, 16, 5, padding=2, bias=True),\n",
    "#     nn.ReLU(),\n",
    "#     nn.BatchNorm2d(16),\n",
    "#     nn.MaxPool2d(2),\n",
    "    \n",
    "    \n",
    "#     Flatten(),\n",
    "#     nn.Linear(16 * 16 * 16, NUM_CLASS),\n",
    "    \n",
    "# )\n",
    "\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train(model, optimizer, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3897\n",
      "Checking accuracy on validation set\n",
      "Got 119 / 1000 correct (11.90)\n",
      "\n",
      "Iteration 100, loss = 2.0391\n",
      "Checking accuracy on validation set\n",
      "Got 268 / 1000 correct (26.80)\n",
      "\n",
      "Iteration 200, loss = 2.4032\n",
      "Checking accuracy on validation set\n",
      "Got 312 / 1000 correct (31.20)\n",
      "\n",
      "Iteration 300, loss = 2.1511\n",
      "Checking accuracy on validation set\n",
      "Got 341 / 1000 correct (34.10)\n",
      "\n",
      "Iteration 0, loss = 1.8867\n",
      "Checking accuracy on validation set\n",
      "Got 336 / 1000 correct (33.60)\n",
      "\n",
      "Iteration 100, loss = 2.1798\n",
      "Checking accuracy on validation set\n",
      "Got 368 / 1000 correct (36.80)\n",
      "\n",
      "Iteration 200, loss = 2.0173\n",
      "Checking accuracy on validation set\n",
      "Got 353 / 1000 correct (35.30)\n",
      "\n",
      "Iteration 300, loss = 1.9002\n",
      "Checking accuracy on validation set\n",
      "Got 331 / 1000 correct (33.10)\n",
      "\n",
      "Iteration 0, loss = 1.9410\n",
      "Checking accuracy on validation set\n",
      "Got 365 / 1000 correct (36.50)\n",
      "\n",
      "Iteration 100, loss = 1.9564\n",
      "Checking accuracy on validation set\n",
      "Got 354 / 1000 correct (35.40)\n",
      "\n",
      "Iteration 200, loss = 1.7017\n",
      "Checking accuracy on validation set\n",
      "Got 358 / 1000 correct (35.80)\n",
      "\n",
      "Iteration 300, loss = 1.8143\n",
      "Checking accuracy on validation set\n",
      "Got 374 / 1000 correct (37.40)\n",
      "\n",
      "Iteration 0, loss = 2.0562\n",
      "Checking accuracy on validation set\n",
      "Got 374 / 1000 correct (37.40)\n",
      "\n",
      "Iteration 100, loss = 1.8783\n",
      "Checking accuracy on validation set\n",
      "Got 363 / 1000 correct (36.30)\n",
      "\n",
      "Iteration 200, loss = 1.7283\n",
      "Checking accuracy on validation set\n",
      "Got 380 / 1000 correct (38.00)\n",
      "\n",
      "Iteration 300, loss = 2.1498\n",
      "Checking accuracy on validation set\n",
      "Got 395 / 1000 correct (39.50)\n",
      "\n",
      "Iteration 0, loss = 1.9232\n",
      "Checking accuracy on validation set\n",
      "Got 377 / 1000 correct (37.70)\n",
      "\n",
      "Iteration 100, loss = 1.9993\n",
      "Checking accuracy on validation set\n",
      "Got 357 / 1000 correct (35.70)\n",
      "\n",
      "Iteration 200, loss = 2.1766\n",
      "Checking accuracy on validation set\n",
      "Got 397 / 1000 correct (39.70)\n",
      "\n",
      "Iteration 300, loss = 1.7022\n",
      "Checking accuracy on validation set\n",
      "Got 392 / 1000 correct (39.20)\n",
      "\n",
      "Iteration 0, loss = 1.8927\n",
      "Checking accuracy on validation set\n",
      "Got 361 / 1000 correct (36.10)\n",
      "\n",
      "Iteration 100, loss = 2.0034\n",
      "Checking accuracy on validation set\n",
      "Got 371 / 1000 correct (37.10)\n",
      "\n",
      "Iteration 200, loss = 1.6719\n",
      "Checking accuracy on validation set\n",
      "Got 409 / 1000 correct (40.90)\n",
      "\n",
      "Iteration 300, loss = 1.8669\n",
      "Checking accuracy on validation set\n",
      "Got 365 / 1000 correct (36.50)\n",
      "\n",
      "Iteration 0, loss = 2.0925\n",
      "Checking accuracy on validation set\n",
      "Got 362 / 1000 correct (36.20)\n",
      "\n",
      "Iteration 100, loss = 1.8340\n",
      "Checking accuracy on validation set\n",
      "Got 405 / 1000 correct (40.50)\n",
      "\n",
      "Iteration 200, loss = 1.9359\n",
      "Checking accuracy on validation set\n",
      "Got 389 / 1000 correct (38.90)\n",
      "\n",
      "Iteration 300, loss = 1.7935\n",
      "Checking accuracy on validation set\n",
      "Got 412 / 1000 correct (41.20)\n",
      "\n",
      "Iteration 0, loss = 2.0431\n",
      "Checking accuracy on validation set\n",
      "Got 400 / 1000 correct (40.00)\n",
      "\n",
      "Iteration 100, loss = 2.3817\n",
      "Checking accuracy on validation set\n",
      "Got 395 / 1000 correct (39.50)\n",
      "\n",
      "Iteration 200, loss = 1.8838\n",
      "Checking accuracy on validation set\n",
      "Got 392 / 1000 correct (39.20)\n",
      "\n",
      "Iteration 300, loss = 1.6847\n",
      "Checking accuracy on validation set\n",
      "Got 413 / 1000 correct (41.30)\n",
      "\n",
      "Iteration 0, loss = 1.7945\n",
      "Checking accuracy on validation set\n",
      "Got 429 / 1000 correct (42.90)\n",
      "\n",
      "Iteration 100, loss = 1.7524\n",
      "Checking accuracy on validation set\n",
      "Got 395 / 1000 correct (39.50)\n",
      "\n",
      "Iteration 200, loss = 2.0059\n",
      "Checking accuracy on validation set\n",
      "Got 413 / 1000 correct (41.30)\n",
      "\n",
      "Iteration 300, loss = 1.7321\n",
      "Checking accuracy on validation set\n",
      "Got 418 / 1000 correct (41.80)\n",
      "\n",
      "Iteration 0, loss = 2.0143\n",
      "Checking accuracy on validation set\n",
      "Got 452 / 1000 correct (45.20)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/THGeneral.cpp:204",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-67ed15b2783d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-b36d68d69509>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epochs, verbose, lr_decay)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# This is the backwards pass: compute the gradient of the loss with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# respect to each  parameter of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/THGeneral.cpp:204"
     ]
    }
   ],
   "source": [
    "channel_0, channel_1, hidden_dim = (32, 64, 300)\n",
    "size_0, size_1 = (3,9)\n",
    "p1, p2 = (0.3, 0.5)\n",
    "\n",
    "print_val = 100\n",
    "\n",
    "\n",
    "\n",
    "reg = 1e-3 # np.random.normal(1e-5,1e-7)\n",
    "learning_rate = 1e-3 # 1e-3 and 5e-4 performs best\n",
    "\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "\n",
    "nn.Conv2d(3, 3, 4, stride = 4, bias=True), # Downsampling\n",
    "nn.BatchNorm2d(3),\n",
    "nn.ReLU(),\n",
    "nn.Dropout2d(p=p1),\n",
    "\n",
    "nn.Conv2d(3, channel_0, size_0, padding = (size_0 - 1) // 2, bias=True),\n",
    "nn.BatchNorm2d(channel_0),\n",
    "nn.ReLU(),\n",
    "nn.Dropout2d(p=p1),\n",
    "nn.MaxPool2d(2),\n",
    "\n",
    "nn.Conv2d(channel_0, channel_1, size_1, padding=(size_1 - 1) // 2, bias=True),\n",
    "nn.BatchNorm2d(channel_1),\n",
    "nn.ReLU(),\n",
    "nn.Dropout2d(p=p1),\n",
    "nn.MaxPool2d(2),\n",
    "\n",
    "Flatten(),\n",
    "\n",
    "nn.Linear(channel_1 * 8 * 8, hidden_dim),\n",
    "nn.BatchNorm1d(hidden_dim),\n",
    "nn.ReLU(),\n",
    "nn.Dropout(p2),\n",
    "\n",
    "nn.Linear(hidden_dim, NUM_CLASS)\n",
    ")\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_history = []\n",
    "\n",
    "train(model, optimizer, epochs = 10, lr_decay = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3658\n",
      "Checking accuracy on validation set\n",
      "Got 100 / 1000 correct (10.00)\n",
      "\n",
      "Iteration 50, loss = 2.0089\n",
      "Iteration 100, loss = 1.9752\n",
      "Checking accuracy on validation set\n",
      "Got 337 / 1000 correct (33.70)\n",
      "\n",
      "Iteration 150, loss = 1.9646\n",
      "Iteration 0, loss = 2.0727\n",
      "Checking accuracy on validation set\n",
      "Got 368 / 1000 correct (36.80)\n",
      "\n",
      "Iteration 50, loss = 1.8265\n",
      "Iteration 100, loss = 1.8963\n",
      "Checking accuracy on validation set\n",
      "Got 394 / 1000 correct (39.40)\n",
      "\n",
      "Iteration 150, loss = 1.9293\n",
      "Iteration 0, loss = 1.9597\n",
      "Checking accuracy on validation set\n",
      "Got 382 / 1000 correct (38.20)\n",
      "\n",
      "Iteration 50, loss = 1.8306\n",
      "Iteration 100, loss = 1.7742\n",
      "Checking accuracy on validation set\n",
      "Got 398 / 1000 correct (39.80)\n",
      "\n",
      "Iteration 150, loss = 1.7587\n",
      "Iteration 0, loss = 1.5588\n",
      "Checking accuracy on validation set\n",
      "Got 419 / 1000 correct (41.90)\n",
      "\n",
      "Iteration 50, loss = 1.7101\n",
      "Iteration 100, loss = 1.8986\n",
      "Checking accuracy on validation set\n",
      "Got 408 / 1000 correct (40.80)\n",
      "\n",
      "Iteration 150, loss = 1.8539\n",
      "Iteration 0, loss = 1.8344\n",
      "Checking accuracy on validation set\n",
      "Got 430 / 1000 correct (43.00)\n",
      "\n",
      "Iteration 50, loss = 1.9101\n",
      "Iteration 100, loss = 1.5530\n",
      "Checking accuracy on validation set\n",
      "Got 436 / 1000 correct (43.60)\n",
      "\n",
      "Iteration 150, loss = 1.7676\n",
      "Iteration 0, loss = 1.6293\n",
      "Checking accuracy on validation set\n",
      "Got 431 / 1000 correct (43.10)\n",
      "\n",
      "Iteration 50, loss = 1.5493\n",
      "Iteration 100, loss = 1.7234\n",
      "Checking accuracy on validation set\n",
      "Got 447 / 1000 correct (44.70)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-364f48c84918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-7a0bdae5f75a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epochs, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# This is the backwards pass: compute the gradient of the loss with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# respect to each  parameter of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reg = 1e-6 # np.random.normal(1e-5,1e-7)\n",
    "learning_rate = 1e-3 # np.random.normal(1e-3,1e-5)\n",
    "channel_0, channel_1, channel_2,channel_3, hidden_dim = (16,32,64,64,500)\n",
    "size_0, size_1, size_2, size_3 = (3,5,7,9)\n",
    "p1, p2 = (0.3, 0.5)\n",
    "\n",
    "print_loss = 50\n",
    "print_val = 100\n",
    "\n",
    "model = nn.Sequential(\n",
    "    \n",
    "    nn.AvgPool2d(4),\n",
    "    \n",
    "    nn.Conv2d(3, channel_0, size_0, padding=(size_0 -1)//2, bias=True),\n",
    "    nn.BatchNorm2d(channel_0),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p=p1),\n",
    "\n",
    "    nn.Conv2d(channel_0, channel_1, size_1, padding=(size_1 -1)//2, bias=True),\n",
    "    nn.BatchNorm2d(channel_1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p=p1),\n",
    "    nn.MaxPool2d(2),\n",
    "\n",
    "    nn.Conv2d(channel_1, channel_2, size_2, padding=(size_2 -1)//2, bias=True),\n",
    "    nn.BatchNorm2d(channel_2), # C in (N, C, H, W)\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p=p1),\n",
    "\n",
    "    nn.Conv2d(channel_2, channel_3, size_3, padding=(size_3 -1)//2, bias=True),\n",
    "    nn.BatchNorm2d(channel_3),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p=p1),\n",
    "    nn.MaxPool2d(2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    nn.Linear(channel_3 * 8 * 8, hidden_dim),\n",
    "    nn.BatchNorm1d(hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p2),\n",
    "\n",
    "    nn.Linear(hidden_dim, NUM_CLASS)\n",
    ")\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "train(model, optimizer, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = np.random.normal(1e-5,1e-7)\n",
    "# learning_rate = np.random.normal(1e-3,1e-5)\n",
    "# channel_0, channel_1, channel_2,channel_3,hidden_dim = (16,32,64,64,500)\n",
    "# size_0, size_1, size_2, size_3 = (3,5,7,9)\n",
    "# p1, p2 = (0.3, 0.5)\n",
    "\n",
    "# model = nn.Sequential(\n",
    "    \n",
    "#     nn.AvePool(4),\n",
    "    \n",
    "#     nn.Conv2d(3, channel_0, size_0, padding=(size_0 -1)//2, bias=True),\n",
    "#     nn.BatchNorm2d(channel_0),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout2d(p=p1),\n",
    "\n",
    "#     nn.Conv2d(channel_0, channel_1, size_1, padding=(size_1 -1)//2, bias=True),\n",
    "#     nn.BatchNorm2d(channel_1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout2d(p=p1),\n",
    "#     nn.MaxPool2d(2),\n",
    "\n",
    "#     nn.Conv2d(channel_1, channel_2, size_2, padding=(size_2 -1)//2, bias=True),\n",
    "#     nn.BatchNorm2d(channel_2), # C in (N, C, H, W)\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout2d(p=p1),\n",
    "\n",
    "#     nn.Conv2d(channel_2, channel_3, size_3, padding=(size_3 -1)//2, bias=True),\n",
    "#     nn.BatchNorm2d(channel_3),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout2d(p=p1),\n",
    "#     nn.MaxPool2d(2),\n",
    "\n",
    "#     Flatten(),\n",
    "\n",
    "#     nn.Linear(channel_3 * 8 * 8, hidden_dim),\n",
    "#     nn.BatchNorm1d(hidden_dim),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p2),\n",
    "\n",
    "#     nn.Linear(hidden_dim, 10)\n",
    "# )\n",
    "\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "#         epochs = 10\n",
    "        \n",
    "#         model = model.to(device=device)  \n",
    "        \n",
    "#         for e in range(epochs):\n",
    "#             scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
    "            \n",
    "#             for t, (x, y) in enumerate(loader_train):\n",
    "#                 model.train()  \n",
    "#                 x = x.to(device=device, dtype=dtype)  \n",
    "#                 y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "#                 scores = model(x)\n",
    "#                 loss = F.cross_entropy(scores, y)\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 loss.backward()\n",
    "\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 if t % print_every == 0:\n",
    "#                     print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "#                     check_accuracy_part34(loader_val, model)\n",
    "#                     val = return_accuracy(loader_val, model)\n",
    "#                     if ( val > best_val):\n",
    "#                         best_val = val\n",
    "#                         best_model = model\n",
    "#                     print()\n",
    "\n",
    "#         val = return_accuracy(loader_val, model)\n",
    "\n",
    "#         if ( val > best_val):\n",
    "#             best_val = val\n",
    "#             best_model = model\n",
    "#             best_params = (dim_spec, learning_rate, reg, dropout_p)\n",
    "            \n",
    "#         print ('######################')\n",
    "#         print ('Finished: ', (dim_spec, learning_rate, reg, dropout_p))\n",
    "        \n",
    "#         print ('######################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1c57c4bcc0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c57cd86a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on validation set\n",
      "Got 459 / 1000 correct (45.90)\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(loader_val, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
