{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 10\n",
    "# Preprocessing the image.\n",
    "# RGB channels' mean and std are hardcoded.\n",
    "# These values are returned from \"channel_statistics.ipynb\" notebook.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.43404748, 0.42429799, 0.40133824), \n",
    "                            (0.26775341, 0.26286543, 0.28041377))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniplacesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, desc_file, root_dir, transform = T.ToTensor()):\n",
    "        \n",
    "        all_data = pd.read_csv(desc_file, delimiter=' ', header = None)\n",
    "        self.desc_frame = all_data.loc[all_data.iloc[:, 1].isin(range(0, NUM_CLASS))]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.desc_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_names = os.path.join(self.root_dir, self.desc_frame.iloc[idx, 0])\n",
    "        images = io.imread(img_names) / 255\n",
    "        \n",
    "        images = self.transform(images)\n",
    "            \n",
    "        return (images, self.desc_frame.iloc[idx,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MiniplacesDataset(desc_file='data/train.txt', root_dir='images/', transform = transform)\n",
    "val_data = MiniplacesDataset(desc_file='data/val.txt', root_dir='images/', transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(train_data, batch_size= 32, shuffle = True)\n",
    "loader_val = DataLoader(val_data, batch_size= 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is modified from Stanford CS231n's assignment 2, Pytorch.ipynb\n",
    "def check_accuracy(loader, model, verbose = False):\n",
    "    print('Checking accuracy on validation set')\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    model.eval()  # set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"Printing first 10 results in each batch\")\n",
    "            print(\"Groud truth: \", y[:20])\n",
    "            print(\"Predicted: \", preds[:20])\n",
    "            \n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is modified from Stanford CS231n's assignment 2, Pytorch.ipynb\n",
    "def train(model, optimizer, epochs=1, verbose = False, lr_decay = 0.9):\n",
    "    \"\"\"    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    - verbose: (Optional, Used for debugging). Print first 10 results in each check for validation accurac\n",
    "    - lr_decay: Exponential decay factor of learning rate for each epoch\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training, and optionally the first 10 results.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)\n",
    "    for e in range(epochs):\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, lr_decay)\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            \n",
    "            model.train()  # put model to training mode\n",
    "            \n",
    "            x = x.to(device=device, dtype=dtype)  \n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_val == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy(loader_val, model, verbose)\n",
    "                print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] \n",
    "    return x.view(N, -1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3897\n",
      "Checking accuracy on validation set\n",
      "Got 119 / 1000 correct (11.90)\n",
      "\n",
      "Iteration 100, loss = 2.0391\n",
      "Checking accuracy on validation set\n",
      "Got 268 / 1000 correct (26.80)\n",
      "\n",
      "Iteration 200, loss = 2.4032\n",
      "Checking accuracy on validation set\n",
      "Got 312 / 1000 correct (31.20)\n",
      "\n",
      "Iteration 300, loss = 2.1511\n",
      "Checking accuracy on validation set\n",
      "Got 341 / 1000 correct (34.10)\n",
      "\n",
      "Iteration 0, loss = 1.8867\n",
      "Checking accuracy on validation set\n",
      "Got 336 / 1000 correct (33.60)\n",
      "\n",
      "Iteration 100, loss = 2.1798\n",
      "Checking accuracy on validation set\n",
      "Got 368 / 1000 correct (36.80)\n",
      "\n",
      "Iteration 200, loss = 2.0173\n",
      "Checking accuracy on validation set\n",
      "Got 353 / 1000 correct (35.30)\n",
      "\n",
      "Iteration 300, loss = 1.9002\n",
      "Checking accuracy on validation set\n",
      "Got 331 / 1000 correct (33.10)\n",
      "\n",
      "Iteration 0, loss = 1.9410\n",
      "Checking accuracy on validation set\n",
      "Got 365 / 1000 correct (36.50)\n",
      "\n",
      "Iteration 100, loss = 1.9564\n",
      "Checking accuracy on validation set\n",
      "Got 354 / 1000 correct (35.40)\n",
      "\n",
      "Iteration 200, loss = 1.7017\n",
      "Checking accuracy on validation set\n",
      "Got 358 / 1000 correct (35.80)\n",
      "\n",
      "Iteration 300, loss = 1.8143\n",
      "Checking accuracy on validation set\n",
      "Got 374 / 1000 correct (37.40)\n",
      "\n",
      "Iteration 0, loss = 2.0562\n",
      "Checking accuracy on validation set\n",
      "Got 374 / 1000 correct (37.40)\n",
      "\n",
      "Iteration 100, loss = 1.8783\n",
      "Checking accuracy on validation set\n",
      "Got 363 / 1000 correct (36.30)\n",
      "\n",
      "Iteration 200, loss = 1.7283\n",
      "Checking accuracy on validation set\n",
      "Got 380 / 1000 correct (38.00)\n",
      "\n",
      "Iteration 300, loss = 2.1498\n",
      "Checking accuracy on validation set\n",
      "Got 395 / 1000 correct (39.50)\n",
      "\n",
      "Iteration 0, loss = 1.9232\n",
      "Checking accuracy on validation set\n",
      "Got 377 / 1000 correct (37.70)\n",
      "\n",
      "Iteration 100, loss = 1.9993\n",
      "Checking accuracy on validation set\n",
      "Got 357 / 1000 correct (35.70)\n",
      "\n",
      "Iteration 200, loss = 2.1766\n",
      "Checking accuracy on validation set\n",
      "Got 397 / 1000 correct (39.70)\n",
      "\n",
      "Iteration 300, loss = 1.7022\n",
      "Checking accuracy on validation set\n",
      "Got 392 / 1000 correct (39.20)\n",
      "\n",
      "Iteration 0, loss = 1.8927\n",
      "Checking accuracy on validation set\n",
      "Got 361 / 1000 correct (36.10)\n",
      "\n",
      "Iteration 100, loss = 2.0034\n",
      "Checking accuracy on validation set\n",
      "Got 371 / 1000 correct (37.10)\n",
      "\n",
      "Iteration 200, loss = 1.6719\n",
      "Checking accuracy on validation set\n",
      "Got 409 / 1000 correct (40.90)\n",
      "\n",
      "Iteration 300, loss = 1.8669\n",
      "Checking accuracy on validation set\n",
      "Got 365 / 1000 correct (36.50)\n",
      "\n",
      "Iteration 0, loss = 2.0925\n",
      "Checking accuracy on validation set\n",
      "Got 362 / 1000 correct (36.20)\n",
      "\n",
      "Iteration 100, loss = 1.8340\n",
      "Checking accuracy on validation set\n",
      "Got 405 / 1000 correct (40.50)\n",
      "\n",
      "Iteration 200, loss = 1.9359\n",
      "Checking accuracy on validation set\n",
      "Got 389 / 1000 correct (38.90)\n",
      "\n",
      "Iteration 300, loss = 1.7935\n",
      "Checking accuracy on validation set\n",
      "Got 412 / 1000 correct (41.20)\n",
      "\n",
      "Iteration 0, loss = 2.0431\n",
      "Checking accuracy on validation set\n",
      "Got 400 / 1000 correct (40.00)\n",
      "\n",
      "Iteration 100, loss = 2.3817\n",
      "Checking accuracy on validation set\n",
      "Got 395 / 1000 correct (39.50)\n",
      "\n",
      "Iteration 200, loss = 1.8838\n",
      "Checking accuracy on validation set\n",
      "Got 392 / 1000 correct (39.20)\n",
      "\n",
      "Iteration 300, loss = 1.6847\n",
      "Checking accuracy on validation set\n",
      "Got 413 / 1000 correct (41.30)\n",
      "\n",
      "Iteration 0, loss = 1.7945\n",
      "Checking accuracy on validation set\n",
      "Got 429 / 1000 correct (42.90)\n",
      "\n",
      "Iteration 100, loss = 1.7524\n",
      "Checking accuracy on validation set\n",
      "Got 395 / 1000 correct (39.50)\n",
      "\n",
      "Iteration 200, loss = 2.0059\n",
      "Checking accuracy on validation set\n",
      "Got 413 / 1000 correct (41.30)\n",
      "\n",
      "Iteration 300, loss = 1.7321\n",
      "Checking accuracy on validation set\n",
      "Got 418 / 1000 correct (41.80)\n",
      "\n",
      "Iteration 0, loss = 2.0143\n",
      "Checking accuracy on validation set\n",
      "Got 452 / 1000 correct (45.20)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/THGeneral.cpp:204",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-67ed15b2783d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-b36d68d69509>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epochs, verbose, lr_decay)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# This is the backwards pass: compute the gradient of the loss with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# respect to each  parameter of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/THGeneral.cpp:204"
     ]
    }
   ],
   "source": [
    "channel_0, channel_1, hidden_dim = (32, 64, 300)\n",
    "size_0, size_1 = (3,9)\n",
    "p1, p2 = (0.3, 0.5)\n",
    "\n",
    "print_val = 100\n",
    "\n",
    "\n",
    "\n",
    "reg = 1e-3 # np.random.normal(1e-5,1e-7)\n",
    "learning_rate = 1e-3 # 1e-3 and 5e-4 performs best\n",
    "\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "\n",
    "nn.Conv2d(3, 3, 4, stride = 4, bias=True), # Downsampling\n",
    "nn.BatchNorm2d(3),\n",
    "nn.ReLU(),\n",
    "nn.Dropout2d(p=p1),\n",
    "\n",
    "nn.Conv2d(3, channel_0, size_0, padding = (size_0 - 1) // 2, bias=True),\n",
    "nn.BatchNorm2d(channel_0),\n",
    "nn.ReLU(),\n",
    "nn.Dropout2d(p=p1),\n",
    "nn.MaxPool2d(2),\n",
    "\n",
    "nn.Conv2d(channel_0, channel_1, size_1, padding=(size_1 - 1) // 2, bias=True),\n",
    "nn.BatchNorm2d(channel_1),\n",
    "nn.ReLU(),\n",
    "nn.Dropout2d(p=p1),\n",
    "nn.MaxPool2d(2),\n",
    "\n",
    "Flatten(),\n",
    "\n",
    "nn.Linear(channel_1 * 8 * 8, hidden_dim),\n",
    "nn.BatchNorm1d(hidden_dim),\n",
    "nn.ReLU(),\n",
    "nn.Dropout(p2),\n",
    "\n",
    "nn.Linear(hidden_dim, NUM_CLASS)\n",
    ")\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "train(model, optimizer, epochs = 10, lr_decay = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3658\n",
      "Checking accuracy on validation set\n",
      "Got 100 / 1000 correct (10.00)\n",
      "\n",
      "Iteration 50, loss = 2.0089\n",
      "Iteration 100, loss = 1.9752\n",
      "Checking accuracy on validation set\n",
      "Got 337 / 1000 correct (33.70)\n",
      "\n",
      "Iteration 150, loss = 1.9646\n",
      "Iteration 0, loss = 2.0727\n",
      "Checking accuracy on validation set\n",
      "Got 368 / 1000 correct (36.80)\n",
      "\n",
      "Iteration 50, loss = 1.8265\n",
      "Iteration 100, loss = 1.8963\n",
      "Checking accuracy on validation set\n",
      "Got 394 / 1000 correct (39.40)\n",
      "\n",
      "Iteration 150, loss = 1.9293\n",
      "Iteration 0, loss = 1.9597\n",
      "Checking accuracy on validation set\n",
      "Got 382 / 1000 correct (38.20)\n",
      "\n",
      "Iteration 50, loss = 1.8306\n",
      "Iteration 100, loss = 1.7742\n",
      "Checking accuracy on validation set\n",
      "Got 398 / 1000 correct (39.80)\n",
      "\n",
      "Iteration 150, loss = 1.7587\n",
      "Iteration 0, loss = 1.5588\n",
      "Checking accuracy on validation set\n",
      "Got 419 / 1000 correct (41.90)\n",
      "\n",
      "Iteration 50, loss = 1.7101\n",
      "Iteration 100, loss = 1.8986\n",
      "Checking accuracy on validation set\n",
      "Got 408 / 1000 correct (40.80)\n",
      "\n",
      "Iteration 150, loss = 1.8539\n",
      "Iteration 0, loss = 1.8344\n",
      "Checking accuracy on validation set\n",
      "Got 430 / 1000 correct (43.00)\n",
      "\n",
      "Iteration 50, loss = 1.9101\n",
      "Iteration 100, loss = 1.5530\n",
      "Checking accuracy on validation set\n",
      "Got 436 / 1000 correct (43.60)\n",
      "\n",
      "Iteration 150, loss = 1.7676\n",
      "Iteration 0, loss = 1.6293\n",
      "Checking accuracy on validation set\n",
      "Got 431 / 1000 correct (43.10)\n",
      "\n",
      "Iteration 50, loss = 1.5493\n",
      "Iteration 100, loss = 1.7234\n",
      "Checking accuracy on validation set\n",
      "Got 447 / 1000 correct (44.70)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-364f48c84918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-7a0bdae5f75a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epochs, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# This is the backwards pass: compute the gradient of the loss with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# respect to each  parameter of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reg = 1e-6 # np.random.normal(1e-5,1e-7)\n",
    "learning_rate = 1e-3 # np.random.normal(1e-3,1e-5)\n",
    "channel_0, channel_1, channel_2,channel_3, hidden_dim = (16,32,64,64,500)\n",
    "size_0, size_1, size_2, size_3 = (3,5,7,9)\n",
    "p1, p2 = (0.3, 0.5)\n",
    "\n",
    "print_loss = 50\n",
    "print_val = 100\n",
    "\n",
    "model = nn.Sequential(\n",
    "    \n",
    "    nn.AvgPool2d(4),\n",
    "    \n",
    "    nn.Conv2d(3, channel_0, size_0, padding=(size_0 -1)//2, bias=True),\n",
    "    nn.BatchNorm2d(channel_0),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p=p1),\n",
    "\n",
    "    nn.Conv2d(channel_0, channel_1, size_1, padding=(size_1 -1)//2, bias=True),\n",
    "    nn.BatchNorm2d(channel_1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p=p1),\n",
    "    nn.MaxPool2d(2),\n",
    "\n",
    "    nn.Conv2d(channel_1, channel_2, size_2, padding=(size_2 -1)//2, bias=True),\n",
    "    nn.BatchNorm2d(channel_2), # C in (N, C, H, W)\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p=p1),\n",
    "\n",
    "    nn.Conv2d(channel_2, channel_3, size_3, padding=(size_3 -1)//2, bias=True),\n",
    "    nn.BatchNorm2d(channel_3),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(p=p1),\n",
    "    nn.MaxPool2d(2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    nn.Linear(channel_3 * 8 * 8, hidden_dim),\n",
    "    nn.BatchNorm1d(hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p2),\n",
    "\n",
    "    nn.Linear(hidden_dim, NUM_CLASS)\n",
    ")\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "train(model, optimizer, epochs = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
